---
title: "Regresja i analiza wariancji - Laboratorium 1"
author:
  name: Adrian Kowalski
  affiliation: Politechnika Krakowska
subtitle: Regresja liniowa wieloraka - wprowadzenie
output:
  html_document:
    df_print: paged
---

# Model regresji liniowej wielorakiej

Do tej pory rozważaliśmy model regresji liniowej prostej $$ y_i = \beta_0 + \beta_1 x_i + \epsilon_i. $$
W rzeczywistości występuje bardzo mało zależności, które da się opisać za pomocą dokładnie jednej zmiennej objaśniającej. Uogólnimy nasze podejście do regresji liniowej wprowadzając nowe zmienne objaśniające i sprowadzając model do postaci.
$$ y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_k x_{ik} + \varepsilon_i.$$
Gdzie $\varepsilon_i$ nadal będą zmiennymi losowymi o rozkładzie $N(0,\sigma^2)$ dla pewnej stałej wariancji $\sigma^2$. Wszelkie pozostałe założenia modelu, które poczyniliśmy przy regresji liniowej prostej pozostają, dojdzie natomiast dodatkowe założenie wynikające z postaci współczynników. Standardowo będziemy oznaczać tak jak poprzednio $$ \hat{y_i} = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_k x_{ik}$$

# Macierzowa postać modelu regresji

Podobnie jak w przypadku regresji liniowej prostej będziemy chcieli zminimalizować błąd średniokwadratowy naszego estymatora. W tym wypadku natomiast przybiera on nieco inną postać
$$ MSE = \frac{\sum_{i=1}^n \varepsilon_i^2}{n - (k + 1)} = \frac{\sum_{i=1}^n (y_i - \hat{y}_i )^2}{n - (k+1)} = \frac{\sum_{i=1}^n (y_i - \beta_0 - \beta_1x_{i1} - \dots - \beta_k x_{ik} )^2}{n - (k+1)}$$
W tym celu spojrzymy na nasz model z nieco innej perspektywy. Spójrzmy jeszcze raz na równanie regresji $$ y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_k x_{ik} + \varepsilon_i. $$
Dla dowolnego ustalonego $i$ możemy je zapisać w formie $$ y_i = [1, x_{i1}, x_{i2}, \dots, x_{ik}]\left[\begin{array}{c} \beta_0 \\ \beta_1 \\ \dots \\ \beta_k \end{array} \right] + \varepsilon_i.$$

Dzięki tej formie zapisu możemy spojrzeć teraz na model całościowo, uwzględniając na raz wszystkie dane.
$$\left[\begin{array}{c} y_1 \\ y_2 \\ \dots \\ y_n \end{array} \right] = \left[\begin{array}{c} 1 \; x_{11} \; \dots x_{1k} \\ 1 \; x_{21} \; \dots \; x_{2k} \\ \dots \\ 1 \; x_{n1} \; \dots \; x_{nk}  \end{array} \right] \left[\begin{array}{c} \beta_0 \\ \beta_1 \\ \dots \\ \beta_k \end{array} \right] + \left[\begin{array}{c} \varepsilon_1 \\ \varepsilon_2 \\ \dots \\ \varepsilon_n \end{array} \right]. $$
Od tej pory będziemy używać zapisu skrótowego
$$ \boldsymbol{y}  = \left[\begin{array}{c} y_1 \\ y_2 \\ \dots \\ y_n \end{array} \right] $$
$$ \boldsymbol{X} = \left[\begin{array}{c} 1 \; x_{11} \; \dots x_{1k} \\ 1 \; x_{21} \; \dots \; x_{2k} \\ \dots \\ 1 \; x_{n1} \; \dots \; x_{nk}  \end{array} \right] $$

$$ \boldsymbol{\beta} = \left[\begin{array}{c} \beta_0 \\ \beta_1 \\ \dots \\ \beta_k \end{array} \right] $$
$$ \boldsymbol{\varepsilon} = \left[\begin{array}{c} \varepsilon_1 \\ \varepsilon_2 \\ \dots \\ \varepsilon_n \end{array} \right]$$
Biorąc pod uwagę wprowadzone oznaczenia, nasz model w skrótowej formie przyjmuje postać $$ \boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}. $$

# Wyznaczenie współczynników modelu liniowego

W celu wyznaczenia ogólnego wzoru na współczynniki modelu regresji liniowej wielorakiej możemy postąpić tak jak poprzednio, po przez minimalizacje błędu średniokwadratowego. Takie rozumowanie prowadzi chociażby autor [tutaj](http://home.iitk.ac.in/~shalab/regression/Chapter3-Regression-MultipleLinearRegressionModel.pdf). Można też wyprowadzić ten wzór za pomocą rozkładu SVD macierzy tak jak robią to autorzy [tutaj](http://databookuw.com/databook.pdf). My skorzystamy ze wzoru na tzw. (lewą) pseudo-odwrotność Moora-Penrose'a $$ X^\dagger = (X^TX)^{-1}X^T$$
Jej zastosowanie daje nam wzór na współczynniki $$ (X^TX)^{-1}X^T\boldsymbol{y} = \beta. $$
Wstawiamy ten wzór do modelu i otrzymujemy $$ \hat{y} = X(X^TX)^{-1}X^T\boldsymbol{y} $$

Macierz $$ H := X(X^TX)^{-1}X^T$$ nazywa się czasem hat matrix.

# Założenia modelu regresji wielorakiej

Zauważmy, że chociażby z faktu, że wykorzystujemy macierz $(X^TX)^{-1}$, ta odwrotność musi istnieć. Musimy zatem dołożyć kolejne założenie do naszej klasycznej listy założeń, mówiące o pełnym rzędzie kolumnowym macierzy $X$, co przekłada się po prostu na brak struktury zależności między zmiennymi objaśniającymi $x_1, x_2, \dots, x_k$. Podsumowując, wykorzystując zapis macierzowy, nasze założenia to

- liniowa zależność między zmienną objaśnianą, a objaśniającą postaci $\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}$,

- wektor losowy $\varepsilon$ ma średnią $\boldsymbol{0}$,

- wektor losowy $\varepsilon$ ma macierz kowariancji postaci $\sigma^2 \boldsymbol{I}$, gdzie $\sigma^2$ to pewna liczba rzeczywista,

- zmienne losowe $\varepsilon_i, \varepsilon_j$ są ze sobą niezależne, dla różnych $i,j$,

- wektor losowy $\varepsilon$ ma wielowymiarowy rozkład normalny $N(0, \sigma^2 \boldsymbol{I})$,

- rzadna ze zmiennych objaśniających nie jest kombinacją liniową pozostałych, innymi słowy, zmienne objaśniające są liniowo niezależne, a macierz $X$ jest pełnego rzędu kolumnowego.


O jakości estymatora, który wyznaczyliśmy w poprzednim rozdziale w sytuacji, gdy powyższe założenia są spełnione mówi twierdzenie [Gaussa-Markowa](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem).

# Współczynnik determinacji $R^2$.

Podobnie jak w przypadku regresji liniowej prostej, możemy w celu oceny jakości modelu skorzystać z współczynnika determinacji $R^2$. Ma on niestety poważną wadę w przypadku regresji liniowej wielorakiej. Przypomnijmy 

$$ R^2 = 1 - \frac{\sum_{i=1}^n \varepsilon_i^2}{\sum_{i=1}^n(y_i - \overline{y})^2}.$$

Dodanie kolejnej zmiennej do modelu nigdy nie podniesie sumy kwadratów reszt, może ją tylko zmniejszyć lub nie zmienić. Zatem $R^2$ będzie większe im więcej zmiennych dodamy do modelu, co nie koniecznie będzie świadczyć o większej jakości modelu. Aby z tym walczyć wprowadzamy tzw. skorygowane $R^2$ (adjusted $R^2$ dane wzorem 
$$ R^2 = 1 - \frac{n - 1}{n - (k+1)}(1 - R^2). $$

Skorygowane $R^2$ bierze pod uwagę jakość modelu i ilość zmiennych objaśniających, więc może być używane do porówynywania różnych modeli.

# Statystyka F

Wywołując summary() na danym modelu regresji liniowej otrzymujemy również wartość statystyki testowej $F$ dla tego modelu. Statystyka ta jest ściśle związana z analizą wariancji i będziemy o niej jeszcze mówić, natomiast warto teraz wspomnieć o hipotezach testu, który ta statystyka wykonuje.
$$ \begin{array}{ll} H_0: & \beta_1 = \beta_2 = \dots = \beta_k = 0 \\ H_1: & \exists_{k} \;  \beta_k \neq 0 \end{array} $$
Innymi słowy, wysoka $p$-wartość testu $F$ świadczy o tym, że nasz model ma wyłącznie zerowe współczynniki.
 
# Ćwiczenie: Model wyceny używanych aut

Skorzystamy z danych [CARS DATASET (Audi, BMW, Ford, Hyundai, Skoda, VW)](https://www.kaggle.com/aishwaryamuthukumar/cars-dataset-audi-bmw-ford-hyundai-skoda-vw). Na podstawie tych danych zbudujemy różne modele regresji liniowej wielorakiej i porównamy je między sobą przy pomocy skorygowanego $R^2$.
```{r}
library(tidyverse)
cars <- readr::read_csv('cars_dataset.csv')
cars <- cars %>% dplyr::mutate(year = 2022 - year) %>% dplyr::rename(age = year)
cars_sample <- dplyr::sample_n(cars, 1000)
cars_sample <- cars_sample %>% mutate(random_col = rnorm(1000, mean = 3, sd = 0.5))
head(cars_sample)
```
```{r}
cars_sample <- cars_sample %>% dplyr::filter((fuelType!='Hybrid')&(fuelType!='Other')&(fuelType!='Electric'))
```

```{r}
cars_model <- lm(price ~ age, data = cars_sample)
summary(cars_model)
```
```{r}
ggplot(cars_sample, aes(x = age)) + geom_histogram(bins = 10)
```
```{r}
cars_model <- lm(price ~ log(age), data=cars_sample)
summary(cars_model)
```
```{r}
cars_model <- lm(price ~ log(age)+mpg, data=cars_sample)
summary(cars_model)
```
```{r}
summary(lm(price ~ log(age)+random_col, data=cars_sample))
```
```{r}
cars_model <- lm(price ~ log(age)+mpg+engineSize, data=cars_sample)
summary(cars_model)
```

```{r}
ggplot(cars_sample, aes(x=engineSize)) + geom_histogram(bins = 20)
```

```{r}
cars_model <- lm(price ~ log(age)+mpg+engineSize+mileage, data=cars_sample)
summary(cars_model)
```

```{r}
round(cor(cars_sample[,c(2,5,7,8,9)]),2)
```

```{r}
library(ggcorrplot)
ggcorrplot(cor(cars_sample[,c(2,5,7,8,9)]), type='lower')
```

```{r}
plot(cars_model, which=1:5)
```