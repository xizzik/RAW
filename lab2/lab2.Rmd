---
title: "Regresja i analiza wariancji - Laboratorium 2"
author:
  name: Adrian Kowalski
  affiliation: Politechnika Krakowska
subtitle: Regresja liniowa prosta - weryfikacja założeń i ocena modelu
output:
  html_document:
    df_print: paged
---

Powróćmy do zbioru danych i modelu z poprzednich zajęć.


```{r, results='hide', message=FALSE}
library(tidyverse)
houses <- readr::read_csv('real_estate.csv')
houses <- houses %>% dplyr::rename(transaction_date = `X1 transaction date`, age = `X2 house age`,
                                   nearest_mrt_distance = `X3 distance to the nearest MRT station`,
                                   stores_no = `X4 number of convenience stores`,
                                   geo_latitude = `X5 latitude`, geo_longtitude = `X6 longitude`,
                                   price_ua = `Y house price of unit area`) %>%
          dplyr::select(!No)
```

```{r}
houses$price_ua <- ((houses$price_ua/3.3)*10000)/28.02 #zamiana na dolar za metr kwadratowy
houses_model <- lm(price_ua ~ nearest_mrt_distance, data=houses)
houses_summary <- summary(houses_model)
houses_summary
```

# Klasyczne założenia modelu regresji

Dokonując analizy regresji czynimi następującą listę założeń, które należy zweryfikować.

1. Między zmienną zależną (egzogeniczną), a niezależną (endogeniczną) istnieje zależność liniowa opisana równaniem $y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$.
2. Reszty modelu, czyli zmienne losowe $\varepsilon_i$ spełniają następującą listę założeń:
- Dla dowolnego $i$ zmienna $\varepsilon_i$ ma rozkład normalny (normalność reszt)
- Dla dowolnego $i$ zmienna $\varepsilon_i$ ma średnią $0$;
- Dla każdego $i$ zmienna $\varepsilon_i$ ma tą samą, stałą wariancję $\sigma^2$ (homoskedastyczność);
- Dla dowolnego $i,j$, jeżeli tylko $i \neq j$ to $\varepsilon_i$ i $\varepsilon_j$ są niezależne (niezależność reszt);




Podsumowując, zakładmy, że reszty modelu $\varepsilon_i \stackrel{iid}{\sim} N(0,\sigma^2)$ dla pewnej liczby $\sigma^2$.

# Założenie 1: Zależność liniowa

Pierwszymi dwoma narzędziamy jakie służą do weryfikacji zależności liniowej jest wykres punktowy oraz współczynnik korelacji Pearsona. Zacznijmy od korelacji Pearsona.
```{r}
cor.test(houses$nearest_mrt_distance, houses$price_ua)
```
Istotny współczynnik korelacji broni założenia o zależności liniowej. Spójrzmy teraz na wykres punktowy i jakie informacje możemy z niego wydobyć.
```{r}
ggplot(houses, aes(x=nearest_mrt_distance, y=price_ua)) + geom_point() +
 labs(title="Wykres punktowy zależnosci ceny metra kwadratowego od odległości od węzła komunikacyjnego", x='Odległość od węzła komuniacyjnego [m]', y = "Cena za metr kwadratowy [$/m^2]") + geom_smooth(method='lm' ,formula=y~x, se=FALSE)
```
Spoglądając na ten wykres nie wykluczamy zależności linowej, należy natomiast zauważyć, że mamy dużo małych wartości i coraz mniej dużych wartości. Do podobnego wniosku doszliśmy poprzednio badając histogramy naszych zmiennych, dla przypomnienia wyglądały następująco.
```{r}
ggplot(houses, aes(x=price_ua)) + geom_histogram(bins=30) + labs(title="Histogram ceny w $ za metr kwadratowy", x='Cena za m^2', y = "Częstotliwość")
ggplot(houses, aes(x=nearest_mrt_distance)) + geom_histogram(bins=30) + labs(title="Histogram odległości od najbliższej stacji transportu masowego", x='Odległość od najbliższej stacji transportu masowego [m]', y = "Częstotliwość")
```
Zauważamy też (ponownie) obserwacje odstające. Tym razem zastosujemy inne podejście do poradzenia sobie z nimi, niż odrzucenie. Sprawdźmy najpierw ile ich jest.
```{r}
ggplot(houses, aes(x=price_ua)) + geom_boxplot(outlier.color='red', outlier.size=5) + 
  labs(title="Wykres pudełkowy ceny za m^2", x='Cena za m^2', y = "") + scale_y_discrete(labels=NULL, breaks=NULL)
```
```{r}
rightmost_value <- quantile(houses$price_ua, 0.75)+1.5*IQR(houses$price_ua)
dplyr::filter(houses, price_ua > rightmost_value)
```
Mamy tylko 3 obserwacje odstające, w sensie rozstępu międzyćwiartkowego. Zastosujemy tzw. clipping, zmieniając ich wartość na skrajnie prawy wąs naszego wykresu pudełkowego.
```{r}
houses <- houses %>% dplyr::mutate(price_ua = ifelse(price_ua > rightmost_value, rightmost_value, price_ua))
ggplot(houses, aes(x=price_ua)) + geom_histogram(bins=30) + labs(title="Histogram ceny w $ za metr kwadratowy (po clippingu)", x='Cena za m^2', y = "Częstotliwość")
```
Dodatkowo ponownie potraktujemy logarytmem zmienną dotyczącą odległości, ze względu na jej wykładniczy charakter.
```{r}
houses <- houses %>% dplyr::mutate(nearest_mrt_distance = log(nearest_mrt_distance))
ggplot(houses, aes(x=nearest_mrt_distance)) + geom_histogram(bins=30) + labs(title="Histogram odległości od najbliższej stacji transportu masowego", x='Odległość od najbliższej stacji transportu masowego [m]', y = "Częstotliwość")
```
Sprawdźmy teraz jak wygląda wykres punktowy.
```{r}
ggplot(houses, aes(x=nearest_mrt_distance, y=price_ua)) + geom_point() +
 labs(title="Wykres punktowy zależnosci ceny metra kwadratowego od odległości od węzła komunikacyjnego", x='Odległość od węzła komuniacyjnego [m]', y = "Cena za metr kwadratowy [$/m^2]") + geom_smooth(method='lm' ,formula=y~x, se=FALSE)
```
Policzmy jeszcze współczynnik korelacji i wykonajmy test statystyczny na to, czy jest on zerowy.
```{r}
cor.test(houses$nearest_mrt_distance, houses$price_ua)
```
Stwórzmy drugi model, już po transformacjach.
```{r}
houses_model <- lm(price_ua ~ nearest_mrt_distance, data=houses)
houses_summary <- summary(houses_model)
houses_summary
```

# Założenie 2: Rozkład reszt

Do zbadania normalności reszt możemy użyć standardowych narzędzi analizy danych, histogramu, wykresu kwartyl-kwartyl oraz testu Shapiro-Wilka.
Na początek wizualne metody.
```{r}
ggplot(houses_model, aes(x=.resid)) + geom_histogram(bins=30) + labs(title='Histogram reszt z modelu', x='Reszty', y='Częstotliwość')
```

```{r}
ggplot(houses_model, aes(sample=.resid)) + geom_qq() + geom_qq_line() + labs(title='Wykres kwartyl-kwartyl reszt', x='Kwartyle teoretyczne', y='Kwartyle próbkowe')
```
Wartości z rozkładu statystyki testowej Shapiro-Wilka nie są łatwe do wyznacznia dokładnego, wyznacza się je za pomocą metody Monte Carlo. Hipotezą zerową testu Shapiro-Wilka jest $H_0:$ Populacja ma rozkład normalny.
```{r}
shapiro.test(houses_model$residuals)
```
Wszystkie metody użyte przez nas świadczą jednoznacznie, że niestety, ale reszty naszego modelu nie mają rozkładu normalnego. Mimo to przejdźmy do weryfikacji dalszych założeń.

# Założenie 3: Zerowa średnia reszt.

Oczywiście do sprawdzenia zerowej średniej reszt możemy użyć klasycznego testu t studenta.

```{r}
t.test(houses_model$residuals)
``` 

W naszym wypadku test t wykazał, że średnia jest równa zero. Warto jednak jeszcze zwrócić uwagę na wykres "Residuals vs Fitted". Przydaje się on nie tylko do określenia czy średnia jest zerowa, ale także do weryfikacji niezależności reszt.

```{r}
plot(houses_model, which=1)
```

Oczywiście powyższy wykres możemy również wykonać przy użyciu ggplot. Szczegóły używania ggplot z obiektami lm() można znaleźć [tutaj](https://ggplot2.tidyverse.org/reference/fortify.lm.html).

```{r}
ggplot(houses_model, aes(.fitted, .resid)) + geom_point() + stat_smooth(method='loess', formula=y~x, se=FALSE) + 
  geom_hline(yintercept=0, linetype='dashed', color='red') + labs(title='Wykres zależności reszt od dopasowanych wartości', x='Dopasowane wartości',
                                                                  y='Reszty')
```

Znaczne odchylenie zaobserwowaliśmy dopiero dla dużych wartości zmiennej objaśnianej.

# Założenie 4: Niezależność reszt

Oprócz powyższego wykresu, dzięki któremu moglibyśmy wykryć strukturę zależnosci w resztach mamy do pomocy test Durbina Watsona. Test Drubina-Watsona bada szczególny rodzaj korelacji, mianowicie zakładamy w nim, że reszty naszego modelu podążają za modelem autokorelacji postaci $\varepsilon_i = \rho \varepsilon_{i-1} + \xi_i$ (gdzie $\xi_i \sim N(0,\sigma^2)$). Hipotezą zerową testu Durbina-Watsona jest $H_0: \; \rho=0$. Innymi słowy hipoteza zerowa tego testu zakłada, że nie ma struktury autoregresyjnej w modelu. W R test Durbina-Watsona znajduje się w pakiecie $\textit{lmtest}$.
```{r, warning=FALSE}
library(lmtest)
lmtest::dwtest(houses_model)
```

W naszym przypadku $p$-wartość jest znacznie większa od $\alpha=0.05$ więc nie mamy dowodów, aby odrzucić hipotezę o niezależności w resztach, co popierałby powyższy wykres.

# Założenie 5: Homoskedastyczność.

W diagnozowaniu homoskedastycznośći przyda nam się wykres "Scale-Location". Przedstawia on zależność pierwiastka ze standaryzowanych reszt od dopasowanych wartości. W sytuacji, gdy nasze reszty mają stałą wariancje, dane powinny być rosproszone losowo względem prostej stale równej $\sigma$.

```{r}
plot(houses_model, which=3)
```

Oczywiście powyższy wykres również możemy odtworzyć przy użyciu ggplot.

```{r}
ggplot(houses_model, aes(.fitted, sqrt(abs(.stdresid)))) + geom_point() + stat_smooth(method='loess', formula=y~x, se=FALSE) +
  labs(title='Zależność pierwiastka standaryzowanych reszt od dopasowanych wartości', x='Dopasowane wartości', y='Pierwiastek standaryzowanych reszt')
```

Z wykresu możnaby wnioskować o zależności liniowej wariancji od dopasowanych wartości. Taką sytuację nazywamy heteroskedastycznością. W celu diagnozowania heteroskedastyczności mamy również narzędzie w postaci testu Breusch-Pagan. Statystykę tego testu konstruujemy jako $n*R_{res}^2$, gdzie $n$-ilość obserwacji, a $R_{res}^2$ jest współczynnikiem modelu $\varepsilon_{i}^2 = \eta_0 + \eta_1 x_i + \xi_i$ (dopasowywujemy nowy model regresji do kwadratów naszych reszt, przy użyciu tych samych zmiennych objaśniających co przy budowaniu pierwotnego modelu). Statystyka testowa ma rozkład $\chi^2$, a hipoteza zerowa $H_0:$ zachodzi homoskedastyczność reszt. 

```{r}
lmtest::bptest(houses_model)
```
$p$-wartość z testu Breusch-Pagan wyniosła zdecydowanie mniej niż $\alpha=0.05$, zatem możemy odrzucić hipotezę zerową o homoskedastyczności reszt.

# Wartości wpływowe, odległość Cook'a i dźwignia.

Poruszymy teraz kwestie obserwacji wpływowych. Obserwacje nazywamy wpływową, jeżeli jej obecność w modelu "zauważalnie" zmienia jego postać. Temat obserwacji wpływowych będziemy jeszcze poruszać, na razie spojrzymy jedynie pierwszy raz na wykresy służące do identyfikacji takich obserwacji oraz dwie podstawowe miary - odległość Cook'a i dźwignię. Dźwignia pomaga nam identyfikować wpływowe wartości zmiennych objaśniających, odległość Cooka natomiast bada obserwacje wpływowe całościowo (uwzględniając zarówno wartość zmiennej objaśniającej i objaśnianej).

```{r}
plot(houses_model, which = 5)
```
```{r}
ggplot(houses_model, aes(.hat, .stdresid)) + geom_point(aes(size=.cooksd)) + stat_smooth(method='loess', formula=y~x, se=FALSE) + labs(title='Leverage vs Standardized Residuals', x='Leverage', y='Standardized Residuals', size='Cooks distance')
```

```{r}
cd <- cooks.distance(houses_model)
cd2 <- cd[cd > 3*mean(cd)]
cd2
as.integer(names(cd2))
```

```{r}
houses <- houses[-as.integer(names(cd2)),]
houses_model <- lm(price_ua ~ nearest_mrt_distance, data=houses)
summary(houses_model)
```
```{r}
plot(houses_model, which=c(1:3,5))
```

```{r}
ggplot(houses, aes(x=nearest_mrt_distance, y=price_ua)) + geom_point() + stat_smooth(method='lm', formula=y~x) + labs(x='', y='')
```

# Ćwiczenie

Zbuduj model regresji liniowej prostej na podstawie zbioru danych Salary Data i zbadaj założenia.

```{r}
salary <- readr::read_csv('Salary_Data.csv')
head(salary)
```




