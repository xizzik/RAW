---
title: "Regresja i analiza wariancji - Laboratorium 8"
subtitle: 'Regresja logistyczna - ocena jakości modelu' 
author: 
  name: 'Adrian Kowalski'
  affiliation: 'Politechnika Krakowska'
output: html_notebook
---

```{r}
library(tidyverse)
breast <- readr::read_csv('breast.csv')
breast <- breast %>% dplyr::select(-c('id','...33')) %>% 
          dplyr::mutate(diagnosis = ifelse(diagnosis == 'M', 1, 0)) %>%
          dplyr::mutate(diagnosis = as.factor(diagnosis))
unique(breast$diagnosis)
```
```{r}
summary(breast)
```

# Zbiór treningowy i testowy

Głównym celem budowy modelu klasyfikacji jest predykcja, w takim razie w naszym interesie jest, aby model dobrze się generalizował. Wobec tego podczas budowy modelu musimy zadbać o wykrycie ewentualnych problemów związanych z brakiem balansu w danych, przeuczeniem i obserwacjami odstającymi. Podczas budowania (dowolnego, ale w szczególności klasyfikacji) modelu należy się kierować zasadą "garbage in - garbage out". Musimy zatem zadbać o to, aby nasze dane były dobrej jakości oraz nie występowały w nich anomalie.

Jednym ze sposobów, warto wspomnieć, że głównym, badania tego jak zbudowany przez nas model będzie się generalizować, jest podział obserwacji na treningowe i testowe. Dane treningowe będą nam służyć podczas procesu uczenia modelu. Dane testowe natomiast nie będą wykorzystywane przy dopasowaniu, będziemy badać na nich wartości wskaźników liczbowych jakości modelu. Najczęściej zbiór treningowy stanowi 3/4 naszych wszystkich obserwacji, ale nie jest to regułą i czasami przyjmuje się inne proporcje. 

Uwaga: Jeżeli nasze dane mają strukturę szeregu czasowego, to należy wziąć pod uwagę przyczynowość, którą to ze sobą wiążę. Wówczas nie możemy podzielić danych na treningowe i testowe w sposób losowy, ze względu na efekt tzw. przecieku danych. Przeciek danych polega na prognozowaniu danych "z przeszłości" za pomocą danych "z przyszłości". 

Uwaga: Przy zadaniu klasyfikacji należy zadbać o to, aby dane w zbiorze treningowym i testowym były zbalansowane.

W środowisku R jest wiele sposobów na podział danych na zbiór treningowy i testowy. Ja zaproponuje użycie pakietu *caret*.

```{r}
library(caret)
train_test_split <- createDataPartition(breast$diagnosis, list = FALSE, p=0.75)
breast_train <- breast[train_test_split,]
breast_test <- breast[-train_test_split,]
cat(dim(breast_train),dim(breast_test))
```
Uwaga: Komenda createDataPartition dba o to, aby balans klas został zachowany taki jak w oryginalnych danych, przez co nie zdarzy się, że w zbiorze testowym znajdą się jedynie obserwacje jednej klasy. Używając innych metod należy o to zadbać!

```{r}
table(breast$diagnosis)
```
```{r}
table(breast_train$diagnosis)
```
```{r}
table(breast_test$diagnosis)
```
Uwaga: Zainteresowanych innymi metodami walki z przeuczeniem odsyłam do zainteresowania się pojęciami "walidacji krzyżowej" i "regularyzacji" (zagadnienia poruszone chociażby w ISLR, ESL, Statystycznych Sytemach uczących się).

# Bazowy model

Zbudujmy prosty bazowy model, na którym będziemy dalej pracować.

```{r}
summary(breast_train)
```
```{r}
first_model <- glm(diagnosis ~ area_mean , data = breast_train, family='binomial')
summary(first_model)
```

# Dokładność

Dokładność (accuracy) jest podstawową charakterystyką liczboWą dowolnego modelu klasyfikacji. Pozwala określić jaka część obserwacji w ogóle została zaklasyfikowana poprawnie. 

Ustalmy na początek notację, przy klasyfikacji binarnej:

- TP - True Positive - obserwacja "prawdziwa" zaklasyfikowana jako prawdziwa,

- TN - True Negative - obserwacja "fałszywa" zaklasyfikowana jako fałszywa,

- FP - False Positive - obserwacja fałszywa zaklasyfikowana jako prawdziwa,

- FN - False Negative - obserwacja prawdziwa zaklasyfikowana jako fałszywa.

Przy takich oznaczeniach dokładność liczymy jako

\[ Acc  = \frac{TP + TN}{TP + TN + FP + FN}.\]

Można ją też wyznaczyć dzieląc ślad macierzy pomyłek przez sumę wszystkich jej elementów.

```{r}
fmpreds <- predict(first_model, breast_test, type = 'response')
fmpreds_classes <- ifelse(fmpreds > 0.5, 1, 0) 
table(fmpreds_classes,breast_test$diagnosis)
```

```{r}
fm_cm <- caret::confusionMatrix(factor(fmpreds_classes), factor(breast_test$diagnosis))
fm_confusion_matrix <- fm_cm$table
fm_confusion_matrix
```
```{r}
round(sum(diag(fm_confusion_matrix))/sum(fm_confusion_matrix), 2)
```
Wiemy już jak liczyć dokładność, czas wspomnieć o jej największej wadzie. Bazowanie swoich wniosków na temat modelu na dokładności jest poprawne pod warunkiem, że dane były zbalansowane. To, że dane są zbalansowane znaczy, że klasy "0" jest w przybliżeniu tyle samo co klasy "1". Jeżeli balans klas nie jest zachowany to należy wziąć pod uwagę wynik tzw. klasyfikatora naiwnego (no information rate).

No information rate to wynik jaki uzyskalibyśmy nie korzystając w żaden sposób z danych innych niż rozkład klas. Wtedy przewidywalibyśmy po prostu tą klasę, którą w danych jest najwięcej. Zatem tę wartość liczymy po prostu jako ilość obserwacji z klasy przeważającej podzieloną przez ilość obserwacji.

```{r}
clbalance <- table(breast_test$diagnosis)
clbalance
```
```{r}
round(clbalance[1]/sum(clbalance),2)
```
# Wizualizacja macierzy pomyłek

Uwaga: W poniższej implementacji wykresu używamy naszej macierzy pomyłek jako obiektu *data frame*, aby móc użyć funkcjonalności *ggplot*, która zlicza nam wystąpienia, czyli *Freq*.

```{r}
ggplot(as.data.frame(fm_confusion_matrix), aes(x = Reference, y = Prediction)) + 
  geom_tile(aes(fill = Freq), colour = 'white') + scale_fill_gradient(low = 'white', high = 'green') +
  geom_text(aes(x = Reference, y = Prediction, label = Freq))
```


# Dewiancja w regresji logistycznej

Zerknijmy jeszcze raz na podsumowanie modelu.

```{r}
summary(first_model)
```
Na samym dole podsumowania znajdujemy parametr zwany dewiancją (deviance). Widzimy dwa takie parametry, dewiancja modelu zerowego (null deviance) i dewiancja reszt (residual deviance). W celu wyznaczenia tych wartośći musimy wprowadzić pojęcie modelu nasyconego (saturated model). Model nasycony to taki, w którym każdej obserwacji odpowiada jeden parametr. W takim razie w modelu nasyconym mamy $n$ parametrów do oszacowania, gdzie $n$ to ilość obserwacji. Model zerowy to taki, który ma dokładnie jeden parametr, niezależny od obserwacji. 

Dewiancję resztową wyliczamy następująco
\[ D = 2(\ell(M_s) - \ell(M_p)). \]
Gdzie

- $\ell$ to log-wiarygodność,

- $M_s$ to model nasycony,

- $M_p$ to model proponowany.

Dewiancję zerową liczymy 

```{r}
saturated <- glm(diagnosis ~ as.factor(1:length(diagnosis)), data = breast_train, family = 'binomial', maxit=100)
2*(logLik(saturated) - logLik(first_model))
```
```{r}
exp(logLik(saturated))
```
```{r}
nullm <- glm(diagnosis ~ 1, data = breast_train, family = 'binomial')
summary(nullm)
```
```{r}
2*(logLik(saturated) - logLik(nullm))
```
Dewiancja modelu zerowego to najgorsza możliwa wartość dewiancji jaką może przyjąć budowany przez nas model pod warunkiem posiadanych przez nas danych.

Za pomocą dewiancji możemy uzyskać dwie informacje. Pierwszą jest wskaźnik $R^2$ McFaddena, który w pewnym sensie mówi nam o jakości modelu. Jest on jedynie jedną z liczbowych charakterystyk modelu i nie ma już takiej interpretacji jak przy modelu regresji liniowej. Jednak jego małe wartości mogą sugerować bardzo zły model. Definiujemy go wzorem

\[ R^2_{pseudo} = 1 - \frac{D}{D_0}. \]
Gdzie $D$ to dewiancja reszt naszego modelu, a $D_0$ dewiancja modelu zerowego.

```{r}
1 - (2*(logLik(saturated) - logLik(first_model)))/(2*(logLik(saturated) - logLik(nullm)))
```

Drugą informacją jest test na istotność różnicy między modelami. Możemy testować różnice dewiancji $D_1$ jednego modelu o $p$ parametrach i dewiancji $D_2$ drugiego modelu o $q$ parametrach. Wtedy $D_1 - D_2$ ma rozkład $\chi^2$ o $p - q$ stopniach swobody (zakładając, że p>q). W szczególności $D - D_0$ ma rozkład $\chi^2$ o $p$ stopniach swobody.

# Czułość i swoistość

Ważnymi charakterystykami liczbowymi podobnymi do dokładności są czułość i swoistość.

Czułość (sensitivity, recall) definiujemy jako

\[ sens = \frac{TP}{TP+FN}. \]

W świetle powyższego wzoru czułość jest stosunkiem poprawnie zaklasyfikowanych zmiennych z klasy "pozytywnej" do ich ilości. Zauważmy, że na czułość już nie ma wpływu zbalansowanie danych, jest ona wręcz często jednym ze znaków świadczących o jego braku.

Swoistość (specificity) jest podobną do czułości wartością, z tym że dotyczącą klasy "negatywnej". Definiujemy ją wzorem

\[ spec = \frac{TN}{TN + FP}. \]

Czyli ponownie mamy stosunek poprawnie zaklasyfikowanych obserwacji z klasy "negatywnej" do ilości obserwacji z tej klasy. 

```{r}
fm_cm
```

# Cwiczenie

Wróćmy do zbioru danych *heart* i dopasujmy do niego "najlepszy" model regresji logistycznej.

```{r}
heart <- readr::read_csv('heart.csv')
```